{
  "id": 1757859334674,
  "question": "You’re designing a data pipeline that ingests semi-structured JSON from a Kafka topic into Snowflake. The data contains nested arrays and optional fields. Downstream consumers require both raw access (for data scientists) and flattened, business-ready tables (for analysts).  How would you:  Ingest and store this data so that schema evolution (e.g., new fields appearing, data types changing) is handled gracefully?  Build transformations that generate flattened tables without breaking when upstream fields are missing, renamed, or nested deeper than expected?  Ensure performance and cost efficiency, given that the pipeline must scale from 1M to 100M daily events?",
  "company": "Seed Question",
  "difficulty": null,
  "topic": "System Design",
  "solution": "```markdown\n1) Ingestion & storage with graceful schema evolution\n\nEvent contracts + registry: Keep producers on Avro/Protobuf with a Schema Registry (e.g., Confluent) and BACKWARD or FULL compatibility. Each record carries a schema_id so we can resolve the writer schema used at publish time.\n\nLanding pattern:\n\nRaw (immutable) “bronze” table in Snowflake with a VARIANT column for the full payload + ingestion metadata (_ingested_at, _topic, _offset, _partition, _key, _schema_id, _source_version).\n\nIngest via Snowflake Kafka Connector or Snowpipe Streaming (for low-latency/cost). If producers insist on JSON, use a JSON FILE FORMAT and keep the entire object in VARIANT.\n\nPartitioning strategy: Physically organize by a reliable event_time (not ingestion time) where possible; add a clustering key on (to_date(event_time), customer_id) to improve pruning.\n\nWhy this handles evolution: New/optional fields simply appear in the VARIANT; old queries don’t break. We never mutate bronze—only append, so we can replay/backfill under new logic anytime.\n\n2) Transformations that don’t break when structure changes\n\nLayered modeling (bronze → silver → gold):\n\nSilver: Normalize and de-duplicate with idempotent MERGE keyed by a stable event key (event_id + optional source checksum). Use TRY_ functions so missing/changed types don’t fail:\n\nTRY_TO_TIMESTAMP(payload:ts) / COALESCE for fallbacks\n\npayload:\"field\"::variant guarded by IFF(HAS_KEY(payload,'field'), ..., NULL)\n\nRobust flattening: Use LATERAL FLATTEN for arrays with ordinal + path columns, and SAFE casts (TRY_TO_NUMBER, TRY_TO_BOOLEAN). For deeply nested surprises, prefer dot-path access with NULL tolerance rather than hard assumptions.\n\nField renames / path drift: Maintain a small path mapping table (e.g., field_name → json_path(s)) and resolve with COALESCE(payload:path_v3, payload:path_v2, payload:path_v1). This centralizes rename logic so SQL and dbt models don’t whack-a-mole.\n\nContract checks: Add dbt tests (or Great Expectations) for minimal contracts: required keys present, enums valid, cardinalities sane. Fail the silver job (not bronze) with clear errors; bronze still ingests.\n\nMaterialization:\n\nUse dbt incremental models or Snowflake Dynamic Tables for declarative, dependency-aware refresh. Pick lag targets (e.g., 5–10 min for near-real-time marts; hourly/daily for heavy marts).\n\nFor wide denormalized “gold”, keep joins deterministic and avoid exploding rows by pre-aggregating array metrics in silver (e.g., item counts, top-N) to reduce flatten cost.\n\n3) Performance & cost (scaling 1M → 100M events/day)\n\nRight ingestion path: Prefer Snowpipe Streaming (lower latency, fewer staged files) or Kafka Connector with sensible batch sizes. If landing files, target 64–256MB compressed objects to optimize micro-partitions.\n\nWarehouse tuning:\n\nSeparate compute pools: small, autosuspend for ELT control queries; medium/large for heavy transforms; a dedicated pool for ad-hoc data science.\n\nEnable auto-suspend/auto-resume and use multi-cluster only when concurrency actually demands it.\n\nUse Query Acceleration only for spiky/complex workloads after other optimizations.\n\nPruning & clustering: Keep event_date in a top-level column for pruning. Automatic Clustering (or periodic reclustering) on (event_date, customer_id) minimizes scan cost as volume grows.\n\nAvoid unnecessary explosion: Flatten late and locally. For analytics that need only aggregates, aggregate in silver first; don’t fully explode giant arrays just to later re-aggregate.\n\nStorage hygiene: Retain bronze indefinitely (cheap + valuable for reprocessing), but time-travel and fail-safe windows on silver/gold can be shorter. Compress/trim verbose free-text.\n\nObservability & SLAs: Track end-to-end lag, row counts, schema_id distribution, error rates, and cost per 1K events. Alert on drifts (e.g., sudden surge of NULL in required business fields).\n```",
  "submittedBy": "dorianganessa",
  "submittedAt": "2025-09-14T14:15:34.674Z",
  "issueNumber": 15
}